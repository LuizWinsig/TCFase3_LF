{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNO+DRJSEjRWAQA9/MzDgc8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaelpivetta/tech-challenge-fase3/blob/main/Running_Llama_3_Chapter_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwQw8uelQwOS",
        "outputId": "2f0cd81b-1d05-459d-bbdc-8823390f4e9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.90.tar.gz (63.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl size=3413681 sha256=df11544d3a6c17000bace7e67764e7fccd33c003c14308444fe33c5b6d2ac9b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/67/02/f950031435db4a5a02e6269f6adb6703bf1631c3616380f3c6\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.90\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d16x9ncSPcWi",
        "outputId": "885106f3-01c2-41d7-bd74-342f60ba2697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n",
            "\n",
            "The review is related to a positive experience of good services, which suggests that it has been received by people who have had experienced satisfied customers.\n",
            "It also mentions that I waited for over an hour, so the Food was not warm.\n"
          ]
        }
      ],
      "source": [
        "# Load the correct class from the library\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"Qwen/Qwen1.5-0.5B-Chat-GGUF\",\n",
        "\tfilename=\"*q8_0.gguf\",\n",
        "  verbose=False # Desabilita logs\n",
        ")\n",
        "\n",
        "#Exemplo 1\n",
        "\n",
        "# output = llm(\n",
        "# \t\t\"Q: What is the circumference of the Earth? A:\", # Prompt\n",
        "#     max_tokens=100,\n",
        "#     stop=[\"Q:\", \"\\n\"],\n",
        "# \t\ttemperature=0.9,\n",
        "#     repeat_penalty=1.1\n",
        "# )\n",
        "\n",
        "# print(output)\n",
        "\n",
        "# # Extract and print the choices\n",
        "# for choice in output['choices']:\n",
        "#     print(choice['text'])\n",
        "\n",
        "#Fim exemplo 1\n",
        "\n",
        "\n",
        "#Exemplo 2\n",
        "\n",
        "# output = llm.create_chat_completion(\n",
        "#     messages = [\n",
        "# \t\t{\n",
        "# \t\t\t\"role\": \"system\",\n",
        "# \t\t\t\"content\": \"You are an assistant who speaks only technicals?\"\n",
        "# \t\t},\n",
        "#     {\n",
        "#       \"role\": \"user\",\n",
        "#       \"content\": \"What is the circumference of the Earth?\"\n",
        "#     }\n",
        "# \t],\n",
        "#     max_tokens=100,\n",
        "#     stop=[\"Q:\", \"\\n\"],\n",
        "# \t\ttemperature=0.9,\n",
        "#     repeat_penalty=1.1\n",
        "# )\n",
        "\n",
        "# print(output)\n",
        "\n",
        "# Extract and print the content\n",
        "# for choice in output['choices']:\n",
        "#     print(choice['message']['content'])\n",
        "\n",
        "#Fim exemplo 2\n",
        "\n",
        "#Exemplo 3\n",
        "\n",
        "# history = [\n",
        "# \t# Instruct the model to behave like Plato\n",
        "# \t{\n",
        "# \t\t\"role\": \"system\",\n",
        "# \t\t\"content\": \"You are the Greek philosopher Plato. Answer every question using his voice.\"\n",
        "# \t},\n",
        "# \t# Identify that the following text is from the user\n",
        "# \t{\n",
        "# \t\t\"role\": \"user\",\n",
        "# \t\t\"content\": \"Can any shape that exist in the real world be perfect and why?\"\n",
        "# \t}\n",
        "# ]\n",
        "# Pass in conversation context to the completion call\n",
        "# output = llm.create_chat_completion(messages=history,\n",
        "#                                     max_tokens=100,\n",
        "#                                     stop=[\"\\n\", \"User:\"],\n",
        "# \t\t                                temperature=0.9,\n",
        "#                                     repeat_penalty=1.3)\n",
        "\n",
        "# print(output)\n",
        "\n",
        "# Extract and print the content\n",
        "# for choice in output['choices']:\n",
        "#     print(choice['message']['content'])\n",
        "\n",
        "#Fim exemplo 3\n",
        "\n",
        "#Exemplo 4\n",
        "# Write the keywords and instructions in the correct locations in the following prompt\n",
        "# text=\"\"\"Instruction: Answer the question in the voice of a pirate, include \"Aye Matey\" in your response.\n",
        "# Question: How long does it take to go around the Earth once?\n",
        "# Response:\n",
        "# \"\"\"\n",
        "\n",
        "# output = llm(\n",
        "#       text,\n",
        "#       max_tokens=32,\n",
        "#       stop=[\"Q:\", \"\\n\"],\n",
        "#       temperature=0.9,\n",
        "#       repeat_penalty=1.3,\n",
        "# )\n",
        "\n",
        "# print(output['choices'][0]['text'])\n",
        "\n",
        "#Fim Exemplo 4\n",
        "\n",
        "# Fill in the 3-shot prompt (you can use multiple lines)\n",
        "text=\"\"\"\n",
        "Review: The delivery was fast, and the food was delicious.\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: I waited for over an hour, and the food was not warm.\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: Great service and the meal was exactly as ordered.\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: Disappointed with Yummy this time. The order arrived late and the food was cold.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "output = llm(text, max_tokens=100,\n",
        "      stop=[\"Q:\"],\n",
        "      temperature=0.9,\n",
        "      repeat_penalty=1.3,\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['text'])\n",
        "\n"
      ]
    }
  ]
}