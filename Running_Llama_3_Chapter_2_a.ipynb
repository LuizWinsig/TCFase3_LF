{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORtlg4lh0xuASWPN56I4Bd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaelpivetta/tech-challenge-fase3/blob/main/Running_Llama_3_Chapter_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIoGXo4wza2d",
        "outputId": "1ea3aad3-6a9c-4b0a-b07e-711ed03e93bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.90.tar.gz (63.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl size=3398645 sha256=fb54e0cefb0ba3f32837192b553c223de8f1773a0f2a4785e58ddb33415db469\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/67/02/f950031435db4a5a02e6269f6adb6703bf1631c3616380f3c6\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.90\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"Qwen/Qwen1.5-0.5B-Chat-GGUF\",\n",
        "\tfilename=\"*q8_0.gguf\",\n",
        "  verbose=False # Desabilita logs\n",
        ")\n",
        "\n",
        "# output = llm(\n",
        "#     \"Q: What galaxy is the closest to us? A:\",\n",
        "#     max_tokens=32,\n",
        "#     stop=[\"Q:\", \"\\n\"],\n",
        "#     # stream=True #Streaming completions\n",
        "# )\n",
        "\n",
        "# print(output[\"choices\"][0][\"text\"])\n",
        "\n",
        "#stream=True\n",
        "# for token in output:\n",
        "#     print(token[\"choices\"][0][\"text\"], end=\"\")\n",
        "\n",
        "# output = llm.create_chat_completion(\n",
        "#     messages=[\n",
        "#         {\n",
        "#             \"role\": \"system\",\n",
        "#             \"content\": \"You are a helpful assistant that outputs in JSON.\"\n",
        "#         },\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": \"Who won the 2020 Nobel prize in physics? Provide a list in 'names'\"\n",
        "#         }\n",
        "#     ],\n",
        "#     response_format={\n",
        "#         \"type\": \"json_object\",\n",
        "#         \"schema\": {\n",
        "#             \"type\": \"object\",\n",
        "#             \"properties\": {\"prize_type\": {\"type\": \"string\"},\n",
        "#                        \"name\": {\"type\": \"string\"}},\n",
        "#             \"required\": [\"prize_type\", \"name\"]}}\n",
        "# )\n",
        "# print(output)\n",
        "# print(output['choices'][0]['message']['content'])\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts item counts from text and outputs it in JSON with the item name as the key and the number of that item as the value\",},\n",
        "        {\"role\": \"user\", \"content\": \"I have fifteen apples, thirty-three oranges, five pineapples, and one hundred fifty-two potatoes.\"},\n",
        "    ],\n",
        "    # Specify output format to JSON\n",
        "    response_format={\n",
        "        \"type\": \"json_array\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"item_name\": {\"type\": \"string\"},\n",
        "                    \"item_count\": {\"type\": \"integer\"}\n",
        "                },\n",
        "                \"required\": [\"item_name\", \"item_count\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# print(output)\n",
        "\n",
        "print(output['choices'][0]['message']['content'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmGjuVQhzj3C",
        "outputId": "ee73d2dc-3487-49c9-8441-42660e7fac40"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the JSON output of the item counts:\n",
            "```json\n",
            "{\n",
            "  \"items\": [\n",
            "    {\n",
            "      \"name\": \"apples\",\n",
            "      \"count\": 15\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"oranges\",\n",
            "      \"count\": 33\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"pineapples\",\n",
            "      \"count\": 5\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"potatoes\",\n",
            "      \"count\": 105\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about space. You return your results in a JSON format with the Question and Answer fields.\",},\n",
        "            {\"role\": \"user\", \"content\": \"How old is the Milky Way Galaxy?\"},\n",
        "        ],\n",
        "        response_format={\n",
        "            \"type\": \"json_object\",\n",
        "          \t# Set the keyword that lets you specify a schema\n",
        "            \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            # Set the properties of the JSON fields and their data types\n",
        "            \"properties\": {\"question\": {\"type\": \"string\"}, \"answer\": {\"type\": \"string\"}},\n",
        "            # Declare the required JSON fields here\n",
        "            \"required\": [\"question\", \"answer\"],\n",
        "            },\n",
        "        },\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBJsT2cWB0jg",
        "outputId": "a0094c34-a019-4b99-9343-591f063fafd2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"question\":\"How old is the Milky Way Galaxy?\", \"answer\":\"The Milky Way Galaxy is approximately 13.8 billion years old.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "    \"Q: What are three names you could give a pirate ship whose crew is looking for an elusive treasure known as the One Piece? A:\",\n",
        "    max_tokens=20,\n",
        "    top_p=0.8\n",
        "    # stream=True #Streaming completions\n",
        ")\n",
        "\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz49dehNFl3V",
        "outputId": "9f98c00b-ceea-43b5-86be-e98563fb9591"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Jack, Robert, and John\n"
          ]
        }
      ]
    }
  ]
}
